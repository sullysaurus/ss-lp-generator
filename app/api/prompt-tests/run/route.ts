import { auth } from "@/app/(auth)/auth";
import { streamText } from "ai";
import { myProvider } from "@/lib/ai/providers";
import { readGuide } from "@/lib/mcp/client";
import { NextResponse } from "next/server";

const GUIDES = [
	{
		uri: "guide://guide1",
		name: "A Respectable Woman",
		number: 1,
	},
	{
		uri: "guide://guide2",
		name: "Teaching Critical Thinking",
		number: 2,
	},
	{
		uri: "guide://guide3",
		name: "The Corrections",
		number: 3,
	},
];

// Map form model values to actual model IDs
const MODEL_MAP: Record<string, string> = {
	grok: "chat-model",
	claude: "chat-model-claude",
	gemini: "chat-model-gemini",
};

export async function POST(request: Request) {
	const session = await auth();

	if (!session?.user?.id) {
		return NextResponse.json({ error: "Unauthorized" }, { status: 401 });
	}

	try {
		const { prompt, model, temperature, maxTokens } = await request.json();

		if (!prompt || !model) {
			return NextResponse.json(
				{ error: "Prompt and model are required" },
				{ status: 400 },
			);
		}

		// Map the form model value to the actual model ID
		const actualModelId = MODEL_MAP[model] || model;

		const results = [];

		// Run the prompt against each guide
		for (const guide of GUIDES) {
			try {
				// Read the guide content
				const guideContent = await readGuide(guide.uri);

				// Create the full prompt with guide context
				const fullPrompt = `${prompt}\n\nGuide Content:\n${guideContent}`;

				// Stream the response
				const { textStream } = streamText({
					model: myProvider.languageModel(actualModelId as any),
					prompt: fullPrompt,
					temperature: temperature || 0.7,
				});

				// Collect the full response
				let fullResponse = "";
				for await (const chunk of textStream) {
					fullResponse += chunk;
				}

				results.push({
					guideName: guide.name,
					guideNumber: guide.number,
					output: fullResponse,
					success: true,
				});
			} catch (error: any) {
				console.error(`Error testing guide ${guide.number}:`, error);
				results.push({
					guideName: guide.name,
					guideNumber: guide.number,
					output: `Error: ${error.message}`,
					success: false,
				});
			}
		}

		// Generate automatic commentary analyzing the outputs
		let commentary = "";
		try {
			const analysisPrompt = `You are evaluating lesson plan outputs generated by an AI model. Analyze how well each output followed the given prompt and provide detailed, constructive commentary.

**Original Prompt:**
${prompt}

**Guide 1 Output (${results[0]?.guideName}):**
${results[0]?.output || "No output"}

**Guide 2 Output (${results[1]?.guideName}):**
${results[1]?.output || "No output"}

**Guide 3 Output (${results[2]?.guideName}):**
${results[2]?.output || "No output"}

**Provide a comprehensive analysis including:**

1. **Overall Assessment** (2-3 sentences)
   - How well did the outputs follow the prompt structure?

2. **Individual Scores** (Format exactly as: "Guide 1: X/10, Guide 2: Y/10, Guide 3: Z/10")
   - Score each from 1-10 on prompt adherence

3. **Strengths**
   - What worked well across outputs?

4. **Areas for Improvement**
   - What could be better?

5. **Prompt Effectiveness**
   - Should the prompt be revised? How?

6. **Key Takeaways**
   - Main insights and recommendations

Format as clear markdown suitable for documentation.`;

			const { textStream } = streamText({
				model: myProvider.languageModel("chat-model-claude"),
				prompt: analysisPrompt,
				temperature: 0.3,
			});

			for await (const chunk of textStream) {
				commentary += chunk;
			}
		} catch (error: any) {
			console.error("Error generating commentary:", error);
			commentary =
				"**Auto-Analysis Failed**\n\nCould not generate automatic commentary. Please add your own notes.";
		}

		return NextResponse.json({
			success: true,
			results,
			commentary,
		});
	} catch (error: any) {
		console.error("Error running prompt test:", error);
		return NextResponse.json(
			{ error: "Failed to run test", details: error.message },
			{ status: 500 },
		);
	}
}
